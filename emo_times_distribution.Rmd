---
title: "My Report"
output: html_document
params:
  rmd_par_island: 'ALL'
  threshold: 20  # ninimal number of reactions to be included in istimation
  num_channels: 20
  t_limit: 12


---

```{r setup, echo=FALSE, message=FALSE}

source("defs.R")


library(lubridate)
library(purrr)
library(ggplot2)
library(MASS)


# must start at 0:0:0 !!!
anal_start_str <- "2025-02-22 00:00:00"
anal_start <- as.POSIXct(anal_start_str)

anal_end_str <- "2025-03-28 23:59:59"
anal_end <- as.POSIXct(anal_end_str)

last_message_str <- anal_end_str
last_messages<- as.POSIXct(last_message_str)

# take care, hole in data from 26.3 - 4.4.  !!!

# 0 nzst be first, 
cs <- c(0,0.125,0.25,0.5,1,2,4,8,params$t_limit)

```


```{r data, echo=FALSE, message=FALSE}

con <- connect_db()

reaction_data <- dbGetQuery(
  con,
  sprintf(
    "SELECT RH.channel_id, RH.message_id, count, RH.TS, posted
     FROM reactions_history RH
     INNER JOIN messages M 
     ON RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
     WHERE RH.reaction_emo = 'VIEWS'
     AND posted > '%s'
     AND posted < '%s'
     AND TS <= '%s'
#     AND RH.message_id = 7621
    ",
    anal_start_str, last_message_str, anal_end_str
  )
)

dbDisconnect(con)


reaction_data <- reaction_data %>%
  arrange(channel_id, message_id, TS) %>%
  group_by(channel_id, message_id) %>%
  mutate(delta_count = count - lag(count, default = 0),
         last_TS = lag(TS))  %>% 
  ungroup()

reaction_data <- reaction_data %>%
  arrange(channel_id, message_id, TS) %>%
  group_by(channel_id, message_id) %>%
  mutate(
    posted_local = with_tz(force_tz(as.POSIXct(posted, tz = "GMT"), "GMT"), "Europe/Prague"),
    time_posted = as.numeric(difftime(as.POSIXct(posted_local), anal_start, units = "hours")) ,
    time_ts = as.numeric(difftime(as.POSIXct(TS), anal_start, units = "hours")))%>%
  mutate(
    time_last_ts = ifelse(is.na(last_TS),time_posted, as.numeric(difftime(as.POSIXct(last_TS), anal_start, units = "hours"))),
#    daytime = format(as.POSIXct(posted_local), "%H:%M:%S"),  # Extract time of day
    daytime_ts = format(as.POSIXct(TS), "%H:%M:%S"),
    daytime_last_ts = format(as.POSIXct(last_TS), "%H:%M:%S")
  ) %>%
  mutate(
    hour_of_day = hour(hms(daytime_ts)) +
                  minute(hms(daytime_ts)) / 60 +
                  second(hms(daytime_ts)) / 3600,
    last_hour_of_day = hour(hms(daytime_last_ts)) +
                  minute(hms(daytime_last_ts)) / 60 +
                  second(hms(daytime_last_ts)) / 3600

  )  %>%
  ungroup() %>%
    dplyr::select(channel_id,message_id, delta_count, posted,TS,time_posted,time_ts, time_last_ts, hour_of_day)

```

```{r seasonality, echo=FALSE, message=FALSE}
library(splines)


# --- Piece-wise linear model ---
seasonal_data <- reaction_data %>% 
   mutate(hour_bin = cut(hour_of_day, breaks = 0:24, right = FALSE, include.lowest = TRUE)) %>%
  filter(time_ts - time_last_ts < 1)

seasonal_model <- lm(delta_count ~ hour_bin, data = seasonal_data)

# Generate prediction data and intervals

# Extract hourly coefficients from the fitted model (run ONCE)
# Extract fitted hourly step values and their SEs
get_hourly_levels <- function(model) {
  # Define hour bins (0–23, midpoint for safety)
  hour_seq <- 0:23 + 0.5
  hour_df <- data.frame(
    hour_of_day = hour_seq,
    hour_bin = cut(hour_seq, breaks = 0:24, right = FALSE, include.lowest = TRUE)
  )

  # Predict with standard errors
  pred <- predict(model, newdata = hour_df, se.fit = TRUE)

  # Prepare named vectors
  values <- setNames(pred$fit, levels(hour_df$hour_bin))
  ses    <- setNames(pred$se.fit, levels(hour_df$hour_bin))

  list(values = values, se = ses)
}


hourly_result <- get_hourly_levels(seasonal_model)

# Access components


norm_c <- 1 /sum(hourly_result$values) * length(hourly_result$values)

hourly_result$values <- hourly_result$values * norm_c
hourly_result$se <-hourly_result$se * norm_c




hourly_values <- hourly_result$values
hourly_ses    <- hourly_result$se

# Build data frame for plotting
hour_df <- data.frame(
  hour = 0:23,
  estimate = as.numeric(hourly_values),
  se = as.numeric(hourly_ses)
) %>%
  mutate(
    ci_lower = estimate - 1.96 * se,
    ci_upper = estimate + 1.96 * se
  )

print(hour_df)

# Bar plot with error bars
ggplot(hour_df, aes(x = factor(hour), y = estimate)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.3) +
  labs(
    title = "Hourly Seasonality Pattern with 95% CI",
    x = "Hour of Day",
    y = "Estimated delta_count"
  ) +
  theme_minimal()


```

```{r view distribution, echo=FALSE, message=FALSE }

library(dplyr)
library(pracma)  # For trapezoidal integration

area_under_seasonality <- function(s, t, hourly_values) {
    stopifnot(is.numeric(s), is.numeric(t), s <= 24 , s >= 0, t <= 24, t >= 0)
   if(s == t)
   {
     return(0)
   }
     if(s >= t)
   {
     return(area_under_seasonality(0, t, hourly_values)+area_under_seasonality(s,24, hourly_values))
   }
  

  # Define helper to get bin index (0-based hour)
  hour_bins <- 0:23
  get_bin <- function(x) floor(x)  # e.g., 8.5 → 8

  # Initialize
  total_area <- 0

  # Current position
  x <- s
  while (x < t) {
    bin <- get_bin(x)
    next_edge <- min(t, bin + 1)
    width <- next_edge - x
    level <- hourly_values[bin + 1]  # +1 for 1-based index
    total_area <- total_area + level * width
    x <- next_edge
  }

  return(total_area)
}

# area_under_seasonality(3,3,hourly_values)


# Compute overlaps properly
compute_overlap <- function(lts, ts, posted, cs) {
  stopifnot(ts-lts < 24, ts >= lts, ts >= posted, lts >= posted)
  overlap_vec <- numeric(k)  # initialize empty vector of length k
  
  for (i in seq_len(k)) {
    lower <- posted + max(cs[i], lts-posted)
    upper <- posted + min(cs[i + 1], ts-posted)
    overlap_vec[i] <- ifelse(lower >= upper,0,   area_under_seasonality(lower %% 24.0,upper %% 24.0 ,hourly_result$values))
#                             upper-lower)
  }
  overlap_vec
  return(overlap_vec)
}

# compute_overlap(0.5,0.7,0.3,cs)

regr_data <- reaction_data  %>%
  mutate( Y = delta_count) %>%
  filter( time_ts - time_posted < params$t_limit )  %>%
  dplyr::select(Y,time_ts,time_last_ts,time_posted)


k <- length(cs) - 1


# Apply to each row of regr_data
X <- do.call(
  rbind,
  lapply(seq_len(nrow(regr_data)), function(j) {
    compute_overlap(regr_data$time_last_ts[j], regr_data$time_ts[j],regr_data$time_posted[j] , cs)
  })
)

# Check rank of X
X_matrix <- as.matrix(X)
rank_X <- qr(X_matrix)$rank
ncol_X <- ncol(X_matrix)

# Print status
cat("Rank of X:", rank_X, " | Number of columns:", ncol_X, "\n")

# Warn if singular
if (rank_X < ncol_X) {
  stop("Design matrix X is singular (columns are linearly dependent).")
}


# Check X
X <- as.data.frame(X)
colnames(X) <- paste0("alpha_", seq_len(k))

# Merge X with Y
model_data <- bind_cols(regr_data, X)


library(purrr)

# Get names of covariates (columns starting with "alpha_")
alpha_vars <- grep("^alpha_", names(model_data), value = TRUE)

# Check if any row has Y > 0 and all alpha_ variables equal to zero
n_rows_ypos_zero_alpha <- model_data %>%
  filter(Y > 0) %>%
  filter(if_all(all_of(alpha_vars), ~ .x == 0)) %>%
  nrow()

if(n_rows_ypos_zero_alpha > 0)
{
  stop("Number of rows with Y > 0 and all alpha_* == 0:", n_rows_ypos_zero_alpha, "\n")
}

# Count for each alpha_* variable the number of rows where it is non-zero
nonzero_counts <- model_data %>%
  summarise(across(all_of(alpha_vars), ~ sum(. != 0)))


print(nonzero_counts)


ols_model <- lm(Y ~ .  - time_ts - time_last_ts - time_posted  + 0, data = model_data)
summary(ols_model)

library(broom)
library(ggplot2)
library(dplyr)

residuals <- resid(ols_model)
fitted_vals <- fitted(ols_model)

# Calculate weights using the absolute value
weights <- 1 / (abs(fitted_vals))
wls_model <- lm(Y ~ . - time_ts - time_last_ts - time_posted+ 0, data = model_data, weights = weights)
summary(wls_model)

# Step 1: Extract WLS model coefficients and CIs
coefs_df <- broom::tidy(wls_model, conf.int = TRUE) %>%
  filter(grepl("^alpha_", term))

# Step 2: Attach x-axis labels cs[2:(k+1)] as categorical variable
k <- length(cs) - 1
cs_labels <- cs[2:(k + 1)]

# Format cs labels (optional: round or pretty format)
cs_labels_formatted <- format(round(cs_labels, 2), nsmall = 2)

# Add labels to coefficients data
coefs_df <- coefs_df %>%
  mutate(
    cs_label = factor(cs_labels_formatted, levels = cs_labels_formatted)
  )

# Step 3: Plot equidistant bar chart
ggplot(coefs_df, aes(x = cs_label, y = estimate)) +
  geom_bar(stat = "identity", fill = "steelblue", width = 0.8) +
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high),
    width = 0.2,
    color = "black"
  ) +
  labs(
    x = "Interval Upper Bound (cs[2] to cs[k+1])",
    y = "WLS Coefficient Estimate",
    title = "WLS Estimates for Interval Coefficients"
  ) +
  theme_minimal(base_size = 14)





```

