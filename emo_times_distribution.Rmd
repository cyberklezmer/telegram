---
title: "Is Anything Authentic on Telegram? May 22nd, 2005"
output: html_document
params:
  rmd_par_island: 'ALL'
  threshold: 20  # ninimal number of reactions to be included in istimation
  num_channels: 20
  t_limit: 16


---

```{r setup, message=FALSE, echo=FALSE, message=FALSE}

source("defs.R")


library(lubridate)
library(purrr)
library(ggplot2)
library(MASS)


# must start at 0:0:0 !!!
anal_start_str <- "2025-05-18 00:00:00"
# anal_start_str <- "2025-02-28 00:00:00"
anal_start <- as.POSIXct(anal_start_str)

anal_end_str <- "2025-05-25 23:59:59"
#anal_end_str <- "2025-02-28 23:59:59"
anal_end <- as.POSIXct(anal_end_str)

last_message_str <- anal_end_str
last_messages<- as.POSIXct(last_message_str)

# take care, hole in data from 26.3 - 4.4.  !!!

# 0 nzst be first, 
cs <- c(0,0.0625/2,0.0625,0.125,0.25,0.5,1,2,4,8,params$t_limit)

```


```{r data, echo=FALSE, message=FALSE, warning=FALSE}

con <- connect_db()

sqlstring <- sprintf(
    "SELECT CH.username, RH.message_id, count, RH.TS, posted, IF(RH.reaction_emo = 'VIEWS','VIEWS',HEX(RH.reaction_emo)) AS emo, CH.quartile, IF(CH.sources LIKE '%%T%%' , 'C',CH.sources) as sources
     FROM reactions_history RH
     INNER JOIN messages M 
     ON RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
     INNER JOIN channels_info CH
     ON RH.channel_id = CH.channel_id 
     WHERE posted > '%s'
     AND posted < '%s'
     AND TS <= '%s'
     AND CH.quartile = 4 
    " ,
    anal_start_str, last_message_str, anal_end_str
  )

# AND   username = 'crypto_insider_deutscher'

reaction_data <- dbGetQuery(
  con, sqlstring
)

# catalogue <- get_catalogue(con)

channel_info <- get_catalogue(con)



invisible(dbDisconnect(con))

reaction_data <- reaction_data %>%
  mutate( sq = paste0(sources,quartile))

reaction_data <- reaction_data %>%
  arrange(username, emo, message_id, TS) %>%
  group_by(username, emo, message_id) %>%
  mutate(delta_count = count - lag(count, default = 0),
         last_TS = lag(TS))  %>% 
  ungroup()

reaction_data <- reaction_data %>%
  arrange(username, message_id, TS) %>%
  group_by(username, message_id) %>%
  mutate(
    posted_local = with_tz(force_tz(as.POSIXct(posted, tz = "GMT"), "GMT"), "Europe/Prague"),
    time_posted = as.numeric(difftime(as.POSIXct(posted_local), anal_start, units = "hours")) ,
    time_ts = as.numeric(difftime(as.POSIXct(TS), anal_start, units = "hours")))%>%
  mutate(
    time_last_ts = ifelse(is.na(last_TS),time_posted, as.numeric(difftime(as.POSIXct(last_TS), anal_start, units = "hours"))),
#    daytime = format(as.POSIXct(posted_local), "%H:%M:%S"),  # Extract time of day
    daytime_ts = format(as.POSIXct(TS), "%H:%M:%S"),
    daytime_last_ts = format(as.POSIXct(last_TS), "%H:%M:%S")
  ) %>%
  mutate(
    hour_of_day = hour(hms(daytime_ts)) +
                  minute(hms(daytime_ts)) / 60 +
                  second(hms(daytime_ts)) / 3600,
    last_hour_of_day = hour(hms(daytime_last_ts)) +
                  minute(hms(daytime_last_ts)) / 60 +
                  second(hms(daytime_last_ts)) / 3600

  )  %>%
  ungroup() %>%
    dplyr::select(username,sources,quartile,sq,message_id,emo, count, delta_count, posted,TS,time_posted,time_ts, time_last_ts, hour_of_day)





usernames <- reaction_data |>
  distinct(username) |>
  pull(username)

sources <- reaction_data |>
  distinct(sources) |>
  pull(sources)

sqs <- reaction_data |>
  distinct(sq) |>
  pull(sq)


```

```{r descstata, echo=FALSE, message=FALSE, warning=FALSE}

# Summarise reaction_data by channel_id
reaction_summary <- reaction_data %>%
  group_by(username) %>%
  summarise(
    n_rows = n(),
    n_messages = n_distinct(message_id),
    n_views = sum(emo == "VIEWS", na.rm = TRUE),
    min_time_posted = min(time_posted, na.rm = TRUE),
    max_time_posted = max(time_posted, na.rm = TRUE),
    .groups = "drop"
  )

# Join the summary back to channel_info (left join to keep all channel_info rows)
channel_summary <- channel_info %>%
  left_join(reaction_summary, by = "username")



day_values <- reaction_data %>%
  mutate( delta_t = time_ts - time_posted, 
          emo_type = ifelse(emo == "VIEWS","view","emo")) %>%
  filter( delta_t >= 23.75 & delta_t <= 24.25)  %>%  
  dplyr::select( username, emo_type, message_id, count)


# Assuming day_values is already created as shown in your code
ve_summary <- day_values %>%
  group_by(username, emo_type) %>%
  summarise(count = sum(count), .groups = "drop") %>%
  pivot_wider(
    names_from = emo_type,
    values_from = count,
    values_fill = 0  # In case one of the types is missing for some users
  )

channel_summary <- channel_summary %>%
  left_join(ve_summary, by = "username")

channel_summary <- channel_summary %>%
    mutate( views_per_sm = view/subscribers,
            emos_per_sm = ifelse(emo == 0, NA, emo/subscribers),
            emos_per_view = ifelse(emo == 0, NA, emo/view))

library(gridExtra) # For arranging plots and tables in a PDF




# Kernel Density Estimation (KDE)

detect_prob_of_outliers <- function(data, variable) {
  # Check if the variable exists in the data frame
  if (!variable %in% names(data)) {
    stop(paste("Variable", variable, "does not exist in the data frame"))
  }
  
  # Check if the variable is numeric
  if (!is.numeric(data[[variable]])) {
    stop(paste("Variable", variable, "is not numeric"))
  }
  
  # Perform kernel density estimation
  density_est <- density(data[[variable]], na.rm = TRUE)
  density_values <- approx(density_est$x, density_est$y, xout = data[[variable]])$y
  
  # Handle any NA density values (e.g., for extreme outliers)
  density_values[is.na(density_values)] <- min(density_values, na.rm = TRUE)
  
  # Assign probabilities (inverse of density for outliers)
  outlier_prob <- 1 / density_values
  outlier_prob <- outlier_prob / max(outlier_prob, na.rm = TRUE)  # Normalize to [0, 1]
  
  outlier_prob[is.na(data[[variable]])] <- NA
  
  # Add the probabilities as a new column in the original data frame
  data[[paste0(variable, "_outlier_prob")]] <- outlier_prob
  
  return(data)
}

# Apply the function to accepted
channel_summary <- detect_prob_of_outliers(channel_summary, "accepted")


# Apply the function to reactions_per_msg_user
channel_summary <- detect_prob_of_outliers(channel_summary, "emos_per_sm")

channel_summary <- detect_prob_of_outliers(channel_summary, "views_per_sm")

channel_summary <- detect_prob_of_outliers(channel_summary, "emos_per_view")

result_data <- channel_summary %>%
    dplyr::select(username,accepted_outlier_prob,emos_per_sm_outlier_prob,views_per_sm_outlier_prob,emos_per_view_outlier_prob)
write.csv(result_data, paste0(output_folder, islandid, "_outliers_res.csv"))

result_data

# messages_data <- get_messages(con)  

# Load necessary libraries

# Calculate summary statistics and outlier counts

summary_stats <- function(data, variable) {
  mean_value <- mean(data[[variable]], na.rm = TRUE)
  variance_value <- var(data[[variable]], na.rm = TRUE)
  num_outliers <- sum(data[[paste0("outlier_", variable)]], na.rm = TRUE)
  return(data.frame(Variable = variable, Mean = mean_value, Variance = variance_value, Outliers = num_outliers))
}


# Summarize data for 'accepted' and 'reactions_per_msg_user'
stats_accepted <- summary_stats(channel_summary, "accepted")
stats_emos_per_sm <- summary_stats(channel_summary, "emos_per_sm")
stats_views_per_sm <- summary_stats(channel_summary, "views_per_sm")
stats_emos_per_view <- summary_stats(channel_summary, "emos_per_view")
summary_table <- rbind(stats_accepted, stats_emos_per_sm, stats_views_per_sm,
                       stats_emos_per_view)


# Write to PDF
pdf(paste0(output_folder, islandid, "_distributions_outliers_with_stats.pdf"))

# Display summary table in the PDF
print(grid.table(summary_table))

 dev.off()

 
library(ggplot2)

plot_histogram <- function(data, column, bins = 20, fill = "lightgreen", 
                           color = "black", alpha = 0.7, title_prefix = "Distribution of") {
  ggplot(data, aes(x = .data[[column]])) +
    geom_histogram(
      bins = bins,
      fill = fill,
      color = color,
      alpha = alpha
    ) +
    labs(
      title = paste(title_prefix, column),
      x = column,
      y = "Frequency"
    ) +
    theme_minimal()
}
 
plot_histogram(channel_summary, "accepted")
plot_histogram(channel_summary, "emos_per_sm")
plot_histogram(channel_summary, "views_per_sm")
plot_histogram(channel_summary, "emos_per_view")



```


### Seasonality

Should be authentic.

```{r seasonality, echo=FALSE, message=FALSE, warning=FALSE}

library(dplyr)

# Function to get and normalize hourly predictions, with consistent factor levels
get_hourly_levels <- function(model) {
  hour_seq <- 0:23 + 0.5

  hour_df <- data.frame(
    hour_of_day = hour_seq,
    hour_bin = cut(hour_seq, breaks = 0:24, right = FALSE, include.lowest = TRUE)
  )

  # Restrict levels to only those used in training
  used_levels <- model$xlevels$hour_bin
  hour_df$hour_bin <- factor(hour_df$hour_bin, levels = used_levels)

  pred <- predict(model, newdata = hour_df, se.fit = TRUE)

  values <- setNames(pred$fit, used_levels)
  ses <- setNames(pred$se.fit, used_levels)

  list(values = values, se = ses)
}

seasonality_pattern <- NULL

compute_seasonality <- function(anal_num, emotic = NULL) {
 
  if(is.null(emotic)) {
    chart_title <- "Views by hour"
  } else {
    chart_title <- paste("Emoticon",ifelse(emotic=="F09F918D","Thumb Up", emotic) ,"by hour")
  }
    
  
  if(anal_num == 0) {
    names <- c("All channels")
  } else if( anal_num == 1) {
    chart_title <- paste(chart_title," by channels")
    names <- usernames
  } else if( anal_num == 2) {
    chart_title <- paste(chart_title," by source and quartile")
    names <- sqs
  }  else if( anal_num == 3) {
    chart_title <- paste(chart_title," by source")
    names <- sources
  }
  # Main loop: create a named list of hour_df-style data frames
  hourly_by_channel <- setNames(names, names) |>
    lapply(function(ch_id) {
      seasonal_data <- reaction_data
      
    if(is.null(emotic)) {
        seasonal_data <-seasonal_data |>
         filter(emo == "VIEWS")
    } else {
        seasonal_data <-seasonal_data |>
         filter(emo == emotic)
    } 
    if(anal_num == 1)
    {
      seasonal_data <-seasonal_data |>
         filter(username == ch_id)
    }
    if(anal_num == 2)
    {
      seasonal_data <-seasonal_data |>
         filter(sq == ch_id)
    }

    if(anal_num == 3)
    {
      seasonal_data <-seasonal_data |>
         filter(sources == ch_id)
    }
              
    seasonal_data  <-seasonal_data |>
        mutate(hour_bin = cut(hour_of_day, breaks = 0:24, right = FALSE, include.lowest = TRUE)) |>
        filter(time_ts - time_last_ts < 1)
  
      if (nrow(seasonal_data) < 5 || length(unique(seasonal_data$hour_bin)) < 2) {
        return(NULL)
      }
  
      model <- lm(delta_count ~ hour_bin, data = seasonal_data)
  
      res <- get_hourly_levels(model)
      
      
      # normalize only on non-NA estimates
      norm_c <- 1 / sum(res$values, na.rm = TRUE) * length(res$values)
      values <- res$values * norm_c
      ses <- res$se * norm_c

      if(anal_num == 0 & is.null(emotic)) {
        seasonality_pattern <<- values
      }
      
        
      data.frame(
        hour = 0:23,
        hour_bin = cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE),
        estimate = as.numeric(values[as.character(cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE))]),
        se = as.numeric(ses[as.character(cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE))])
      ) |>
        mutate(
          ci_lower = estimate - 1.96 * se,
          ci_upper = estimate + 1.96 * se
        )
    })
  
  
  hourly_long_df <- hourly_by_channel |>
    discard(is.null) |>
    imap_dfr(~ mutate(.x, username = .y))
  
                                                                                                              
  p <- ggplot(hourly_long_df, aes(x = factor(hour), y = estimate)) +
    geom_col(fill = "steelblue", width = 0.9) +
    geom_errorbar(
      aes(ymin = ci_lower, ymax = ci_upper),
      width = 0.25,
      color = "black"
    ) +
    facet_wrap(~ username, scales = "free_y") +
    labs(
      title = chart_title,
      x = "Hour of Day",
      y = "Normalized Estimate"
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )

  return(p)
}

p<- compute_seasonality(0)

p


```

Languages: C - Czech and Slovak, G - German, P - Polish

```{r seasonality_sensitivity, echo=FALSE, message=FALSE, warning=FALSE }

# p<- compute_seasonality(2)

# p


# p<- compute_seasonality(2,"F09F918D")

# p



p<- compute_seasonality(3)

p

p<- compute_seasonality(0,"F09F918D")

p

p<- compute_seasonality(3,"F09F918D")

p



```

### Views and Emoticons

Should be authentic.



```{r view distribution, echo=FALSE, message=FALSE, warning=FALSE }

library(dplyr)
library(pracma)  # For trapezoidal integration


area_under_seasonality <- function(s, t, hourly_values) {
    stopifnot(is.numeric(s), is.numeric(t), s <= 24 , s >= 0, t <= 24, t >= 0)
   if(s == t)
   {
     return(0)
   }
     if(s >= t)
   {
     return(area_under_seasonality(0, t, hourly_values)+area_under_seasonality(s,24, hourly_values))
   }
  

  # Define helper to get bin index (0-based hour)
  hour_bins <- 0:23
  get_bin <- function(x) floor(x)  # e.g., 8.5 → 8

  # Initialize
  total_area <- 0

  # Current position
  x <- s
  while (x < t) {
    bin <- get_bin(x)
    next_edge <- min(t, bin + 1)
    width <- next_edge - x
    level <- hourly_values[bin + 1]  # +1 for 1-based index
    total_area <- total_area + level * width
    x <- next_edge
  }

  return(total_area)
}



# Compute overlaps properly
# needs seasonality_pattern from seasonality

compute_overlap <- function(lts, ts, posted, cs) {
  stopifnot(ts-lts < 24, ts >= lts, ts >= posted, lts >= posted)
  overlap_vec <- numeric(k)  # initialize empty vector of length k
  
  for (i in seq_len(k)) {
    lower <- posted + max(cs[i], lts-posted)
    upper <- posted + min(cs[i + 1], ts-posted)
    overlap_vec[i] <- ifelse(lower >= upper,0,   area_under_seasonality(lower %% 24.0,upper %% 24.0 ,seasonality_pattern))
#                             upper-lower)
  }
  return(overlap_vec)
}

# compute_overlap(0.5,0.7,0.3,cs)

regr_data <- reaction_data  %>%
  mutate( Y = delta_count) %>%
  filter( time_ts - time_posted < params$t_limit & emo == "VIEWS")  %>%
  dplyr::select(Y,time_ts,time_last_ts,time_posted,username,sources,sq)

emo_regr_data <- reaction_data  %>%
  mutate( Y = delta_count) %>%
  filter( time_ts - time_posted < params$t_limit & emo != "VIEWS")  %>%
  dplyr::select(Y,time_ts,time_last_ts,time_posted,username,sources,sq)

k <- length(cs) - 1



library(dplyr)
library(purrr)
library(broom)
library(ggplot2)

skipped_users <- list()

fit_user_model <- function(user_data, cs, k) {
  crit <- unique(user_data$crit)

  # Show number of rows
  message("Processing: ", crit, " (", nrow(user_data), " rows)")

  tryCatch({
    # Build design matrix
    X <- do.call(
      rbind,
      lapply(seq_len(nrow(user_data)), function(j) {
        compute_overlap(
          user_data$time_last_ts[j],
          user_data$time_ts[j],
          user_data$time_posted[j],
          cs
        )
      })
    )

    X_matrix <- as.matrix(X)
    rank_X <- qr(X_matrix)$rank
    ncol_X <- ncol(X_matrix)

    message("  Rank of X: ", rank_X, " / ", ncol_X)

    # Proceed even if X is rank-deficient — let lm() fail naturally
    X <- as.data.frame(X)
    colnames(X) <- paste0("alpha_", seq_len(k))
    model_data <- bind_cols(user_data, X)
    alpha_vars <- grep("^alpha_", names(model_data), value = TRUE)
    ypos_zero_alpha <- model_data |>
      filter(Y > 0) |>
      filter(if_all(all_of(alpha_vars), ~ .x == 0))

    if (nrow(ypos_zero_alpha) > 0) {
      message("  Skipping ", crit, ": Y > 0 with all alpha_* == 0 (", nrow(ypos_zero_alpha), " rows)")
      return(NULL)
    }
#    model_data <- read.csv("tmp/simonevoss_md.csv")
    # Fit OLS and get 
    model_data <- model_data |>
      dplyr::select(Y, starts_with("alpha_"))  # Keep Y + regressors only

    write.csv(model_data,paste0("tmp/",crit, "_md.csv"))
    ols_model <- lm(
      Y ~ . + 0,
#     Y ~ alpha_1 + alpha_2 + 0,
     data = model_data
    )
    weights <- 1 / abs(fitted(ols_model))
    # Fit WLS
    wls_model <- lm(
      Y ~ . + 0,
      data = model_data,
      weights = weights
    )

    coefs_df <- tidy(wls_model, conf.int = TRUE) |>
      filter(grepl("^alpha_", term))

    cs_labels <- cs[2:(k + 1)]
    cs_labels_formatted <- format(round(cs_labels, 2), nsmall = 2)
    
      interval_lengths <- cs[2:(k + 1)] - cs[1:k]
      
      coefs_df <- coefs_df |>
        mutate(
          cs_label = factor(cs_labels_formatted, levels = cs_labels_formatted),
          crit = crit,
          pre_prob = interval_lengths * estimate,
          pre_prob_low = interval_lengths * conf.low,
          pre_prob_high = interval_lengths * conf.high
        ) |>
        mutate(
          prob = pre_prob / sum(pre_prob),
          prob_low = pre_prob_low / sum(pre_prob),
          prob_high = pre_prob_high / sum(pre_prob)
        )


  }, error = function(e) {
    message("  Error for ", crit, ": ", e$message)
    return(NULL)
  })
}


dist_group <- function(anal_num, data) {
  
  if(anal_num == 0)  
  {
     r_data <- data %>%
       mutate(crit = "all")
  }
  else if(anal_num == 1) {
     r_data <- data %>%
       mutate(crit = username)
  } else if(anal_num == 2) {
     r_data <- data %>%
       mutate(crit = sources)
  } else if(anal_num == 3) {
     r_data <- data %>%
       mutate(crit = sq)
  } else if(anal_num == 4) {
     r_data <- data %>%
       filter(sources == "P") %>%
       mutate(crit = username)  
  }

  user_list <- split(r_data, r_data$crit)
  
  user_coefs_list <- purrr::map(user_list, fit_user_model, cs = cs, k = k) |>
  purrr::compact()

  return(user_coefs_list)
}

plot_res <- function(user_coefs_list, title)
{
  plot_df <- bind_rows(user_coefs_list)

  p<-ggplot(plot_df, aes(x = cs_label, y = prob)) +
  geom_bar(stat = "identity", fill = "steelblue", width = 0.8) +
  geom_errorbar(aes(ymin = prob_low, ymax = prob_high),
                width = 0.2, color = "black") +
  labs(
    x = "hours",
    y = "p",
    title = paste("Distribution of ",title,"time")
  ) +
  facet_wrap(~ crit, scales = "free_y", ncol = 5) +
  theme_minimal(base_size = 8)
 return(p)
}


all_coefs_list <- dist_group(0, regr_data)
plot_res(all_coefs_list,"view")

all_emo_coefs_list <- dist_group(0, emo_regr_data)
plot_res(all_emo_coefs_list,"emo")

# sources_coefs_list <- dist_group(2)
# plot_res(sources_coefs_list)

s_coefs_list <- dist_group(2, regr_data)
plot_res(s_coefs_list,"view")

s_emo_coefs_list <- dist_group(2, emo_regr_data)
plot_res(s_emo_coefs_list,"emo")

pl_coefs_list <- dist_group(4, regr_data)
plot_res(pl_coefs_list,"view")

pl_emo_coefs_list <- dist_group(4, emo_regr_data)
plot_res(pl_coefs_list,"emo")


# Step 6: Plot as facet grid
# ggsave("out/viewtimes.pdf",p, height = 10)

```


```{r emo_vs_view , echo=FALSE, message=FALSE, warning=FALSE}

channels_coefs_list <- dist_group(1,regr_data)

channels_emo_coefs_list <- dist_group(1,emo_regr_data)


```

```{r forwards, echo=FALSE, message=FALSE, warning=FALSE}

library(binom)
library(DBI)

# Connect to your MySQL database
con <- connect_db()

# Fetch table data
data <- dbGetQuery(
  con,"SELECT forwards.*, channels_info.username FROM forwards INNER JOIN channels_info ON forwards.channel_id = channels_info.channel_id WHERE channels_info.quartile = 4")

data <- data |>
  add_count(username, name = "n") |>
  filter(n >= 50) |>
  dplyr::select(-n)

invisible(dbDisconnect(con))

# Compute the time difference in hours
data <- data |>
  filter(!is.na(posted), !is.na(src_posted)) |>
  mutate(
    delay_hours = as.numeric(difftime(posted, src_posted, units = "hours"))
  ) |>
  filter(!is.na(delay_hours))

# Create histogram bins per channel
histograms_by_channel <- data |>
  filter(!is.na(posted), !is.na(src_posted)) |>
  mutate(delay_hours = as.numeric(difftime(posted, src_posted, units = "hours"))) |>
  mutate(bin = cut(delay_hours, breaks = cs, include.lowest = TRUE, right = FALSE)) |>
  filter(!is.na(bin)) |>  # Remove undefined bins (outside cs)
  group_by(username, bin, .drop = FALSE) |>
  summarise(count = n(), .groups = "drop") |>
  group_by(username) |>
  mutate(
    total = sum(count),
    prop = count / total
  ) |>
  rowwise() |>
  mutate(
    ci = list(binom::binom.confint(count, total, method = "wilson"))
  ) |>
  unnest_wider(ci) |>
  dplyr::select(username, bin, count, prop, lower, upper) |>
  group_by(username) |>
  group_split() |>
  setNames(
    data |>
      filter(!is.na(posted), !is.na(src_posted)) |>
      mutate(delay_hours = as.numeric(difftime(posted, src_posted, units = "hours"))) |>
      mutate(bin = cut(delay_hours, breaks = cs, include.lowest = TRUE, right = FALSE)) |>
      filter(!is.na(bin)) |>
      group_by(username) |>
      group_keys() |>
      pull(username)
  )




```


### Forwards

May be inauthentic

Forwards - blue, views - red

```{r combining, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=50}

plot_combined_histogram <- function(username) {
  # Get actual histogram
  actual_df <- histograms_by_channel[[username]] |>
    mutate(
      bin_label = as.character(bin),
      source = "Forwards"
    ) |>
    rename(prob = prop, prob_low = lower, prob_high = upper)

  # Ensure consistent bin ordering
  bin_levels <- actual_df$bin_label

  # Parse modeled terms like "alpha_1", "alpha_2", ...
  modeled_df <- all_coefs_list[["all"]] |>
    mutate(
      bin_index = as.integer(gsub("alpha_", "", term)),
      bin_label = bin_levels[bin_index],
      source = "Views"
    ) |>
    dplyr::select(bin_label, prob, prob_low, prob_high, source)

  # Combine actual and modeled data
  combined_df <- bind_rows(actual_df, modeled_df) |>
    mutate(bin_label = factor(bin_label, levels = bin_levels))

  # Plot with dodged bars for actual and modeled values
  ggplot(combined_df, aes(x = bin_label, y = prob, fill = source)) +
    geom_col(position = position_dodge(width = 0.9), width = 0.8) +
    geom_errorbar(
      aes(ymin = prob_low, ymax = prob_high),
      position = position_dodge(width = 0.9),
      width = 0.2
    ) +
    labs(
      title = username,
      x = "Time delay bin",
      y = "Probability"
    ) +
    scale_fill_manual(values = c("Forwards" = "steelblue", "Views" = "red")) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 10, face = "bold"),
      axis.text.x = element_text(size = 6, angle = 45, hjust = 1),
      axis.text.y = element_text(size = 6),
      axis.title = element_text(size = 8),
      plot.margin = margin(2, 2, 2, 2),
      legend.position = "none"  # ❌ Hides the legend
    )
  }


# Ensure ggplot2 and grDevices are loaded
library(ggplot2)
library(grDevices)

library(patchwork)

# Generate all plots into a list
plot_list <- lapply(names(histograms_by_channel), plot_combined_histogram)

# Combine them into a grid with 3 columns
wrap_plots(plot_list, ncol = 3)

```
