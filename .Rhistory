proportion = num_msgs / total_msgs,
msg_count_rank = dense_rank(-num_msgs)
) %>%
arrange(entropy, msg_count_rank) %>%
ungroup()
# Function to generate histogram for a specific username
plot_histogram <- function(username, data) {
ggplot(data %>% filter(username == !!username), aes(x = msg_count_rank, y = proportion)) +
geom_bar(stat = "identity", fill = "blue", color = "black") +
ggtitle(paste( username, " : ")) +
xlab("Message Count Rank") +
ylab("Proportion of Total Messages")
}
# Get unique usernames
usernames <- unique(plot_df$username) %>% head(20)
# Generate a list of plots
plot_list <- lapply(usernames, function(user) {
plot_histogram(user, plot_df)
})
# Save plots to PDF
pdf(paste(output_folder,islandid,"_top_entropy_histograms.pdf"), width = 11, height = 28.5) # Adjust dimensions if needed
grid.arrange(grobs = plot_list, ncol = 2) # Adjust ncol for grid arrangement
dev.off()
dbDisconnect(con)
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(dplyr)
library(ggplot2)
library(entropy)
library(gridExtra)
# Function to calculate entropy
calculate_entropy <- function(counts, K) {
n <- sum(counts)
p <- counts / n
# MLE (empirical) entropy in natural log
H_mle <- -sum(p * log(p), na.rm = TRUE)
# Miller–Madow correction term
#   H_mm <- H_mle + (K - 1) / (2 * n)
H_penalized <- H_mle + 5 / n
return(H_mm)
}
# Data preprocessing
source("defs.R")
con <- connect_db()
messages_df <- dbGetQuery(con,"SELECT channel_id, src_channel_id, count(msg_id) as num_msgs FROM messages WHERE src_channel_id IS NOT NULL GROUP BY channel_id, src_channel_id")
K <- n_distinct(messages_df$src_channel_id)
# Join the data
# Compute entropy for each channel_id
entropy_data <- messages_df %>%
group_by(channel_id) %>%
summarize(
entropy = calculate_entropy(num_msgs,K),
nsources = n_distinct(src_channel_id),
forwarded_msgs = sum(num_msgs),
.groups = "drop"
)
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(dplyr)
library(ggplot2)
library(entropy)
library(gridExtra)
# Function to calculate entropy
calculate_entropy <- function(counts, K) {
n <- sum(counts)
p <- counts / n
# MLE (empirical) entropy in natural log
H_mle <- -sum(p * log(p), na.rm = TRUE)
# Miller–Madow correction term
#   H_mm <- H_mle + (K - 1) / (2 * n)
H_penalized <- H_mle + 5 / n
return(H_penalized)
}
# Data preprocessing
source("defs.R")
con <- connect_db()
messages_df <- dbGetQuery(con,"SELECT channel_id, src_channel_id, count(msg_id) as num_msgs FROM messages WHERE src_channel_id IS NOT NULL GROUP BY channel_id, src_channel_id")
K <- n_distinct(messages_df$src_channel_id)
# Join the data
# Compute entropy for each channel_id
entropy_data <- messages_df %>%
group_by(channel_id) %>%
summarize(
entropy = calculate_entropy(num_msgs,K),
nsources = n_distinct(src_channel_id),
forwarded_msgs = sum(num_msgs),
.groups = "drop"
)
catalogue_data <- get_catalogue(con,islandcondition)
entropy_data <- entropy_data %>%
left_join(catalogue_data, by = "channel_id")  %>%
arrange(entropy)
# Assuming messages_df is your data frame with columns channel_id and msg_count
# If not already done, group the data and calculate the frequencies
# Calculate proportions for msg_count and preprocess data
plot_df <- messages_df %>%
left_join(entropy_data, by = "channel_id") %>%
dplyr::select(username, entropy, num_msgs) %>%
group_by(username) %>%
mutate(
total_msgs = sum(num_msgs),
proportion = num_msgs / total_msgs,
msg_count_rank = dense_rank(-num_msgs)
) %>%
arrange(entropy, msg_count_rank) %>%
ungroup()
# Function to generate histogram for a specific username
plot_histogram <- function(username, data) {
ggplot(data %>% filter(username == !!username), aes(x = msg_count_rank, y = proportion)) +
geom_bar(stat = "identity", fill = "blue", color = "black") +
ggtitle(paste( username, " : ")) +
xlab("Message Count Rank") +
ylab("Proportion of Total Messages")
}
# Get unique usernames
usernames <- unique(plot_df$username) %>% head(20)
# Generate a list of plots
plot_list <- lapply(usernames, function(user) {
plot_histogram(user, plot_df)
})
# Save plots to PDF
pdf(paste(output_folder,islandid,"_top_entropy_histograms.pdf"), width = 11, height = 28.5) # Adjust dimensions if needed
grid.arrange(grobs = plot_list, ncol = 2) # Adjust ncol for grid arrangement
dev.off()
dbDisconnect(con)
write.csv(entropy_Data,paste0(output_folder,islandid,"_entropy_data.csv"))
write.csv(entropy_data,paste0(output_folder,islandid,"_entropy_data.csv"))
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(dplyr)
library(ggplot2)
library(entropy)
library(gridExtra)
# Function to calculate entropy
calculate_entropy <- function(counts, K) {
n <- sum(counts)
p <- counts / n
# MLE (empirical) entropy in natural log
H_mle <- -sum(p * log(p), na.rm = TRUE)
# Miller–Madow correction term
#   H_mm <- H_mle + (K - 1) / (2 * n)
H_penalized <- H_mle + 5 / n
return(H_penalized)
}
# Data preprocessing
source("defs.R")
con <- connect_db()
messages_df <- dbGetQuery(con,"SELECT channel_id, src_channel_id, count(msg_id) as num_msgs FROM messages WHERE src_channel_id IS NOT NULL GROUP BY channel_id, src_channel_id")
K <- n_distinct(messages_df$src_channel_id)
# Join the data
# Compute entropy for each channel_id
entropy_data <- messages_df %>%
group_by(channel_id) %>%
summarize(
entropy = calculate_entropy(num_msgs,K),
nsources = n_distinct(src_channel_id),
forwarded_msgs = sum(num_msgs),
.groups = "drop"
)
catalogue_data <- get_catalogue(con,islandcondition)
entropy_data <- entropy_data %>%
left_join(catalogue_data, by = "channel_id")  %>%
arrange(entropy)
write.csv(entropy_data,paste0(output_folder,islandid,"_entropy_data.csv"))
# Assuming messages_df is your data frame with columns channel_id and msg_count
# If not already done, group the data and calculate the frequencies
# Calculate proportions for msg_count and preprocess data
plot_df <- messages_df %>%
left_join(entropy_data, by = "channel_id") %>%
dplyr::select(username, entropy, num_msgs) %>%
group_by(username) %>%
mutate(
total_msgs = sum(num_msgs),
proportion = num_msgs / total_msgs,
msg_count_rank = dense_rank(-num_msgs)
) %>%
arrange(entropy, msg_count_rank) %>%
ungroup()
# Function to generate histogram for a specific username
plot_histogram <- function(username, data) {
ggplot(data %>% filter(username == !!username), aes(x = msg_count_rank, y = proportion)) +
geom_bar(stat = "identity", fill = "blue", color = "black") +
ggtitle(paste( username, " : ")) +
xlab("Message Count Rank") +
ylab("Proportion of Total Messages")
}
# Get unique usernames
usernames <- unique(plot_df$username) %>% head(60)
# Generate a list of plots
plot_list <- lapply(usernames, function(user) {
plot_histogram(user, plot_df)
})
# Save plots to PDF
pdf(paste(output_folder,islandid,"_top_entropy_histograms.pdf"), width = 11, height = 50) # Adjust dimensions if needed
grid.arrange(grobs = plot_list, ncol = 4) # Adjust ncol for grid arrangement
dev.off()
dbDisconnect(con)
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(gridExtra) # For arranging plots and tables in a PDF
source("defs.R")
con <- connect_db()
catalogue_data <- get_catalogue(con,islandcondition)
# Convert data into long format for ggplot2 compatibility
# Calculate the IQR and detect upper outliers for accepted
# Define a function to calculate upper bounds and detect outliers
detect_outliers <- function(data, variable) {
q1 <- quantile(data[[variable]], 0.25, na.rm = TRUE)
q3 <- quantile(data[[variable]], 0.75, na.rm = TRUE)
iqr <- q3 - q1
upper_bound <- q3 + 1.5 * iqr
data[[paste0("outlier_", variable)]] <- data[[variable]] > upper_bound
return(list(data = data, upper_bound = upper_bound))
}
# Example Data
detect_prob_of_outliers <- function(data, variable) {
# Check if the variable exists in the data frame
if (!variable %in% names(data)) {
stop(paste("Variable", variable, "does not exist in the data frame"))
}
# Check if the variable is numeric
if (!is.numeric(data[[variable]])) {
stop(paste("Variable", variable, "is not numeric"))
}
# Perform kernel density estimation
density_est <- density(data[[variable]], na.rm = TRUE)
density_values <- approx(density_est$x, density_est$y, xout = data[[variable]])$y
# Handle any NA density values (e.g., for extreme outliers)
density_values[is.na(density_values)] <- min(density_values, na.rm = TRUE)
# Assign probabilities (inverse of density for outliers)
outlier_prob <- 1 / density_values
outlier_prob <- outlier_prob / max(outlier_prob, na.rm = TRUE)  # Normalize to [0, 1]
# Add the probabilities as a new column in the original data frame
data[[paste0(variable, "_outlier_prob")]] <- outlier_prob
return(data)
}
# Apply the function to accepted
result <- detect_outliers(catalogue_data, "accepted")
catalogue_data <- result$data
accepted_upper <- result$upper_bound
# Apply the function to reactions_per_msg_user
data <- detect_prob_of_outliers(data, "reactions_per_msg_user")
names(data
names(data)
names(data)
catalogue_data <- detect_prob_of_outliers(catalogue_data, "reactions_per_msg_user")
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(gridExtra) # For arranging plots and tables in a PDF
source("defs.R")
con <- connect_db()
catalogue_data <- get_catalogue(con,islandcondition)
# Convert data into long format for ggplot2 compatibility
# Calculate the IQR and detect upper outliers for accepted
# Define a function to calculate upper bounds and detect outliers
detect_outliers <- function(data, variable) {
q1 <- quantile(data[[variable]], 0.25, na.rm = TRUE)
q3 <- quantile(data[[variable]], 0.75, na.rm = TRUE)
iqr <- q3 - q1
upper_bound <- q3 + 1.5 * iqr
data[[paste0("outlier_", variable)]] <- data[[variable]] > upper_bound
return(list(data = data, upper_bound = upper_bound))
}
# Example Data
detect_prob_of_outliers <- function(data, variable) {
# Check if the variable exists in the data frame
if (!variable %in% names(data)) {
stop(paste("Variable", variable, "does not exist in the data frame"))
}
# Check if the variable is numeric
if (!is.numeric(data[[variable]])) {
stop(paste("Variable", variable, "is not numeric"))
}
# Perform kernel density estimation
density_est <- density(data[[variable]], na.rm = TRUE)
density_values <- approx(density_est$x, density_est$y, xout = data[[variable]])$y
# Handle any NA density values (e.g., for extreme outliers)
density_values[is.na(density_values)] <- min(density_values, na.rm = TRUE)
# Assign probabilities (inverse of density for outliers)
outlier_prob <- 1 / density_values
outlier_prob <- outlier_prob / max(outlier_prob, na.rm = TRUE)  # Normalize to [0, 1]
# Add the probabilities as a new column in the original data frame
data[[paste0(variable, "_outlier_prob")]] <- outlier_prob
return(data)
}
# Apply the function to accepted
catalogue_data <- detect_prob_of_outliers(catalogue_data, "accepted")
result <- detect_outliers(catalogue_data, "accepted")
catalogue_data <- result$data
accepted_upper <- result$upper_bound
# Apply the function to reactions_per_msg_user
catalogue_data <- detect_prob_of_outliers(catalogue_data, "reactions_per_msg_user")
result <- detect_outliers(catalogue_data, "reactions_per_msg_user")
catalogue_data <- result$data
reactions_upper <- result$upper_bound
# Display the dataset with outlier flags
write.csv(catalogue_data, paste0(output_folder, islandid, "_outliers.csv"))
# messages_data <- get_messages(con)
# Load necessary libraries
# Calculate summary statistics and outlier counts
summary_stats <- function(data, variable) {
mean_value <- mean(data[[variable]], na.rm = TRUE)
variance_value <- var(data[[variable]], na.rm = TRUE)
num_outliers <- sum(data[[paste0("outlier_", variable)]], na.rm = TRUE)
return(data.frame(Variable = variable, Mean = mean_value, Variance = variance_value, Outliers = num_outliers))
}
# Summarize data for 'accepted' and 'reactions_per_msg_user'
stats_accepted <- summary_stats(catalogue_data, "accepted")
stats_reactions <- summary_stats(catalogue_data, "reactions_per_msg_user")
summary_table <- rbind(stats_accepted, stats_reactions)
# Write to PDF
pdf(paste0(output_folder, islandid, "_distributions_outliers_with_stats.pdf"))
# Display summary table in the PDF
print(grid.table(summary_table))
# Plot distribution for 'accepted'
print(
ggplot(catalogue_data, aes(x = accepted)) +
geom_histogram(bins = 20, fill = "lightgreen", color = "black", alpha = 0.7) +
geom_vline(xintercept = accepted_upper, color = "red", linetype = "dashed") +
labs(title = "Distribution of accepted", x = "accepted", y = "Frequency") +
theme_minimal()
)
# Plot distribution for 'reactions_per_msg_user'
print(
ggplot(catalogue_data, aes(x = reactions_per_msg_user)) +
geom_histogram(bins = 20, fill = "orange", color = "black", alpha = 0.7) +
geom_vline(xintercept = reactions_upper, color = "red", linetype = "dashed") +
labs(title = "Distribution of reactions_per_msg_user", x = "reactions_per_msg_user", y = "Frequency") +
theme_minimal()
)
# Close the PDF
dev.off()
# Close the database connectionint
dbDisconnect(con)
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(gridExtra) # For arranging plots and tables in a PDF
source("defs.R")
con <- connect_db()
catalogue_data <- get_catalogue(con,islandcondition)
# Convert data into long format for ggplot2 compatibility
# Calculate the IQR and detect upper outliers for accepted
# Define a function to calculate upper bounds and detect outliers
detect_outliers <- function(data, variable) {
q1 <- quantile(data[[variable]], 0.25, na.rm = TRUE)
q3 <- quantile(data[[variable]], 0.75, na.rm = TRUE)
iqr <- q3 - q1
upper_bound <- q3 + 1.5 * iqr
data[[paste0("outlier_", variable)]] <- data[[variable]] > upper_bound
return(list(data = data, upper_bound = upper_bound))
}
# Kernel Density Estimation (KDE)
detect_prob_of_outliers <- function(data, variable) {
# Check if the variable exists in the data frame
if (!variable %in% names(data)) {
stop(paste("Variable", variable, "does not exist in the data frame"))
}
# Check if the variable is numeric
if (!is.numeric(data[[variable]])) {
stop(paste("Variable", variable, "is not numeric"))
}
# Perform kernel density estimation
density_est <- density(data[[variable]], na.rm = TRUE)
density_values <- approx(density_est$x, density_est$y, xout = data[[variable]])$y
# Handle any NA density values (e.g., for extreme outliers)
density_values[is.na(density_values)] <- min(density_values, na.rm = TRUE)
# Assign probabilities (inverse of density for outliers)
outlier_prob <- 1 / density_values
outlier_prob <- outlier_prob / max(outlier_prob, na.rm = TRUE)  # Normalize to [0, 1]
outlier_prob <- ifelse(is.na(outlier_prob),NA, outlier_prob)
# Add the probabilities as a new column in the original data frame
data[[paste0(variable, "_outlier_prob")]] <- outlier_prob
return(data)
}
# Apply the function to accepted
catalogue_data <- detect_prob_of_outliers(catalogue_data, "accepted")
result <- detect_outliers(catalogue_data, "accepted")
catalogue_data <- result$data
accepted_upper <- result$upper_bound
# Apply the function to reactions_per_msg_user
catalogue_data <- detect_prob_of_outliers(catalogue_data, "reactions_per_msg_user")
result <- detect_outliers(catalogue_data, "reactions_per_msg_user")
catalogue_data <- result$data
reactions_upper <- result$upper_bound
# Display the dataset with outlier flags
write.csv(catalogue_data, paste0(output_folder, islandid, "_outliers.csv"))
# messages_data <- get_messages(con)
# Load necessary libraries
# Calculate summary statistics and outlier counts
summary_stats <- function(data, variable) {
mean_value <- mean(data[[variable]], na.rm = TRUE)
variance_value <- var(data[[variable]], na.rm = TRUE)
num_outliers <- sum(data[[paste0("outlier_", variable)]], na.rm = TRUE)
return(data.frame(Variable = variable, Mean = mean_value, Variance = variance_value, Outliers = num_outliers))
}
# Summarize data for 'accepted' and 'reactions_per_msg_user'
stats_accepted <- summary_stats(catalogue_data, "accepted")
stats_reactions <- summary_stats(catalogue_data, "reactions_per_msg_user")
summary_table <- rbind(stats_accepted, stats_reactions)
# Write to PDF
pdf(paste0(output_folder, islandid, "_distributions_outliers_with_stats.pdf"))
# Display summary table in the PDF
print(grid.table(summary_table))
# Plot distribution for 'accepted'
print(
ggplot(catalogue_data, aes(x = accepted)) +
geom_histogram(bins = 20, fill = "lightgreen", color = "black", alpha = 0.7) +
geom_vline(xintercept = accepted_upper, color = "red", linetype = "dashed") +
labs(title = "Distribution of accepted", x = "accepted", y = "Frequency") +
theme_minimal()
)
# Plot distribution for 'reactions_per_msg_user'
print(
ggplot(catalogue_data, aes(x = reactions_per_msg_user)) +
geom_histogram(bins = 20, fill = "orange", color = "black", alpha = 0.7) +
geom_vline(xintercept = reactions_upper, color = "red", linetype = "dashed") +
labs(title = "Distribution of reactions_per_msg_user", x = "reactions_per_msg_user", y = "Frequency") +
theme_minimal()
)
# Close the PDF
dev.off()
# Close the database connectionint
dbDisconnect(con)
# identifies top channles by reach, evaluates distribution of accepting/forwarding and makes a graph of proximity
# number of channels in graph and stats
library(gridExtra) # For arranging plots and tables in a PDF
source("defs.R")
con <- connect_db()
catalogue_data <- get_catalogue(con,islandcondition)
# Convert data into long format for ggplot2 compatibility
# Calculate the IQR and detect upper outliers for accepted
# Define a function to calculate upper bounds and detect outliers
detect_outliers <- function(data, variable) {
q1 <- quantile(data[[variable]], 0.25, na.rm = TRUE)
q3 <- quantile(data[[variable]], 0.75, na.rm = TRUE)
iqr <- q3 - q1
upper_bound <- q3 + 1.5 * iqr
data[[paste0("outlier_", variable)]] <- data[[variable]] > upper_bound
return(list(data = data, upper_bound = upper_bound))
}
# Kernel Density Estimation (KDE)
detect_prob_of_outliers <- function(data, variable) {
# Check if the variable exists in the data frame
if (!variable %in% names(data)) {
stop(paste("Variable", variable, "does not exist in the data frame"))
}
# Check if the variable is numeric
if (!is.numeric(data[[variable]])) {
stop(paste("Variable", variable, "is not numeric"))
}
# Perform kernel density estimation
density_est <- density(data[[variable]], na.rm = TRUE)
density_values <- approx(density_est$x, density_est$y, xout = data[[variable]])$y
# Handle any NA density values (e.g., for extreme outliers)
density_values[is.na(density_values)] <- min(density_values, na.rm = TRUE)
# Assign probabilities (inverse of density for outliers)
outlier_prob <- 1 / density_values
outlier_prob <- outlier_prob / max(outlier_prob, na.rm = TRUE)  # Normalize to [0, 1]
outlier_prob[is.na(data[[variable]])] <- NA
# Add the probabilities as a new column in the original data frame
data[[paste0(variable, "_outlier_prob")]] <- outlier_prob
return(data)
}
# Apply the function to accepted
catalogue_data <- detect_prob_of_outliers(catalogue_data, "accepted")
result <- detect_outliers(catalogue_data, "accepted")
catalogue_data <- result$data
accepted_upper <- result$upper_bound
# Apply the function to reactions_per_msg_user
catalogue_data <- detect_prob_of_outliers(catalogue_data, "reactions_per_msg_user")
result <- detect_outliers(catalogue_data, "reactions_per_msg_user")
catalogue_data <- result$data
reactions_upper <- result$upper_bound
# Display the dataset with outlier flags
write.csv(catalogue_data, paste0(output_folder, islandid, "_outliers.csv"))
# messages_data <- get_messages(con)
# Load necessary libraries
# Calculate summary statistics and outlier counts
summary_stats <- function(data, variable) {
mean_value <- mean(data[[variable]], na.rm = TRUE)
variance_value <- var(data[[variable]], na.rm = TRUE)
num_outliers <- sum(data[[paste0("outlier_", variable)]], na.rm = TRUE)
return(data.frame(Variable = variable, Mean = mean_value, Variance = variance_value, Outliers = num_outliers))
}
# Summarize data for 'accepted' and 'reactions_per_msg_user'
stats_accepted <- summary_stats(catalogue_data, "accepted")
stats_reactions <- summary_stats(catalogue_data, "reactions_per_msg_user")
summary_table <- rbind(stats_accepted, stats_reactions)
# Write to PDF
pdf(paste0(output_folder, islandid, "_distributions_outliers_with_stats.pdf"))
# Display summary table in the PDF
print(grid.table(summary_table))
# Plot distribution for 'accepted'
print(
ggplot(catalogue_data, aes(x = accepted)) +
geom_histogram(bins = 20, fill = "lightgreen", color = "black", alpha = 0.7) +
geom_vline(xintercept = accepted_upper, color = "red", linetype = "dashed") +
labs(title = "Distribution of accepted", x = "accepted", y = "Frequency") +
theme_minimal()
)
# Plot distribution for 'reactions_per_msg_user'
print(
ggplot(catalogue_data, aes(x = reactions_per_msg_user)) +
geom_histogram(bins = 20, fill = "orange", color = "black", alpha = 0.7) +
geom_vline(xintercept = reactions_upper, color = "red", linetype = "dashed") +
labs(title = "Distribution of reactions_per_msg_user", x = "reactions_per_msg_user", y = "Frequency") +
theme_minimal()
)
# Close the PDF
dev.off()
# Close the database connectionint
dbDisconnect(con)
