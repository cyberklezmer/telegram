rbind,
lapply(seq_len(nrow(regr_data)), function(j) {
compute_overlap(
regr_data$time_last_ts[j],
regr_data$time_ts[j],
regr_data$time_posted[j],
cs
)
})
)
X_global <- as.data.frame(X_global)
colnames(X_global) <- paste0("alpha_", seq_len(k))
model_data_global <- bind_cols(regr_data, X_global)
ols_global <- lm(
Y ~ . - time_ts - time_last_ts - time_posted - username + 0,
data = model_data_global
)
weights_global <- 1 / abs(fitted(ols_global))
wls_global <- lm(
Y ~ . - time_ts - time_last_ts - time_posted - username + 0,
data = model_data_global,
weights = weights_global
)
coefs_global <- tidy(wls_global, conf.int = TRUE) |>
filter(grepl("^alpha_", term)) |>
mutate(
cs_label = factor(format(round(cs[2:(k + 1)], 2), nsmall = 2)),
username = "ALL"
)
# Step 5: Combine all coefficients
coefs_all <- bind_rows(coefs_user, coefs_global)
p
# Step 6: Plot as facet grid
p<-ggplot(coefs_all, aes(x = cs_label, y = estimate)) +
geom_bar(stat = "identity", fill = "steelblue", width = 0.8) +
geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
width = 0.2, color = "black") +
labs(
x = "Interval Upper Bound (cs[2] to cs[k+1])",
y = "WLS Coefficient Estimate",
title = "WLS Estimates for Interval Coefficients (per User)"
) +
facet_wrap(~ username, scales = "free_y") +
theme_minimal(base_size = 14)
p
ggsave(p,"out/viewtimes.pdf")
ggsave("out/viewtimes.pdf",p)
p<-ggplot(coefs_all, aes(x = cs_label, y = estimate)) +
geom_bar(stat = "identity", fill = "steelblue", width = 0.8) +
geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
width = 0.2, color = "black", ncol = 5) +
labs(
x = "Interval Upper Bound (cs[2] to cs[k+1])",
y = "WLS Coefficient Estimate",
title = "WLS Estimates for Interval Coefficients (per User)"
) +
facet_wrap(~ username, scales = "free_y") +
theme_minimal(base_size = 14)
p
ggsave("out/viewtimes.pdf",p)
p<-ggplot(coefs_all, aes(x = cs_label, y = estimate)) +
geom_bar(stat = "identity", fill = "steelblue", width = 0.8) +
geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
width = 0.2, color = "black") +
labs(
x = "Interval Upper Bound (cs[2] to cs[k+1])",
y = "WLS Coefficient Estimate",
title = "WLS Estimates for Interval Coefficients (per User)"
) +
facet_wrap(~ username, scales = "free_y", ncol = 5) +
theme_minimal(base_size = 14)
p
ggsave("out/viewtimes.pdf",p)
p<-ggplot(coefs_all, aes(x = cs_label, y = estimate)) +
geom_bar(stat = "identity", fill = "steelblue", width = 0.8) +
geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
width = 0.2, color = "black") +
labs(
x = "Interval Upper Bound (cs[2] to cs[k+1])",
y = "WLS Coefficient Estimate",
title = "WLS Estimates for Interval Coefficients (per User)"
) +
facet_wrap(~ username, scales = "free_y", ncol = 5) +
theme_minimal(base_size = 8)
p
ggsave("out/viewtimes.pdf",p)
ggsave("out/viewtimes.pdf",p, height = 20)
ggsave("out/viewtimes.pdf",p, height = 10)
source("defs.R")
library(lubridate)
library(purrr)
library(ggplot2)
library(MASS)
# must start at 0:0:0 !!!
anal_start_str <- "2025-02-22 00:00:00"
anal_start <- as.POSIXct(anal_start_str)
anal_end_str <- "2025-02-28 23:59:59"
anal_end <- as.POSIXct(anal_end_str)
last_message_str <- anal_end_str
last_messages<- as.POSIXct(last_message_str)
# take care, hole in data from 26.3 - 4.4.  !!!
# 0 nzst be first,
cs <- c(0,0.0625,0.125,0.25,0.5,1,2,4,8,params$t_limit)
con <- connect_db()
reaction_data <- dbGetQuery(
con,
sprintf(
"SELECT CH.username, RH.message_id, count, RH.TS, posted, IF(RH.reaction_emo = 'VIEWS','VIEWS',HEX(RH.reaction_emo)) AS emo, CH.quartile, CH.sources
FROM reactions_history RH
INNER JOIN messages M
ON RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
INNER JOIN channels_info CH
ON RH.channel_id = CH.channel_id
#     WHERE RH.reaction_emo = 'VIEWS'
AND posted > '%s'
AND posted < '%s'
AND TS <= '%s'
#     AND RH.message_id = 7621
",
anal_start_str, last_message_str, anal_end_str
)
)
# catalogue <- get_catalogue(con)
dbDisconnect(con)
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(delta_count = count - lag(count, default = 0),
last_TS = lag(TS))  %>%
ungroup()
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(
posted_local = with_tz(force_tz(as.POSIXct(posted, tz = "GMT"), "GMT"), "Europe/Prague"),
time_posted = as.numeric(difftime(as.POSIXct(posted_local), anal_start, units = "hours")) ,
time_ts = as.numeric(difftime(as.POSIXct(TS), anal_start, units = "hours")))%>%
mutate(
time_last_ts = ifelse(is.na(last_TS),time_posted, as.numeric(difftime(as.POSIXct(last_TS), anal_start, units = "hours"))),
#    daytime = format(as.POSIXct(posted_local), "%H:%M:%S"),  # Extract time of day
daytime_ts = format(as.POSIXct(TS), "%H:%M:%S"),
daytime_last_ts = format(as.POSIXct(last_TS), "%H:%M:%S")
) %>%
mutate(
hour_of_day = hour(hms(daytime_ts)) +
minute(hms(daytime_ts)) / 60 +
second(hms(daytime_ts)) / 3600,
last_hour_of_day = hour(hms(daytime_last_ts)) +
minute(hms(daytime_last_ts)) / 60 +
second(hms(daytime_last_ts)) / 3600
)  %>%
ungroup() %>%
dplyr::select(username,message_id, delta_count, posted,TS,time_posted,time_ts, time_last_ts, hour_of_day)
usernames <- reaction_data |>
distinct(username) |>
pull(username)
sources <- reaction_data |>
distinct(sources) |>
pull(sources)
con <- connect_db()
reaction_data <- dbGetQuery(
con,
sprintf(
"SELECT CH.username, RH.message_id, count, RH.TS, posted, IF(RH.reaction_emo = 'VIEWS','VIEWS',HEX(RH.reaction_emo)) AS emo, CH.quartile, CH.sources
FROM reactions_history RH
INNER JOIN messages M
ON RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
INNER JOIN channels_info CH
ON RH.channel_id = CH.channel_id
#     WHERE RH.reaction_emo = 'VIEWS'
AND posted > '%s'
AND posted < '%s'
AND TS <= '%s'
#     AND RH.message_id = 7621
",
anal_start_str, last_message_str, anal_end_str
)
)
# catalogue <- get_catalogue(con)
dbDisconnect(con)
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(delta_count = count - lag(count, default = 0),
last_TS = lag(TS))  %>%
ungroup()
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(
posted_local = with_tz(force_tz(as.POSIXct(posted, tz = "GMT"), "GMT"), "Europe/Prague"),
time_posted = as.numeric(difftime(as.POSIXct(posted_local), anal_start, units = "hours")) ,
time_ts = as.numeric(difftime(as.POSIXct(TS), anal_start, units = "hours")))%>%
mutate(
time_last_ts = ifelse(is.na(last_TS),time_posted, as.numeric(difftime(as.POSIXct(last_TS), anal_start, units = "hours"))),
#    daytime = format(as.POSIXct(posted_local), "%H:%M:%S"),  # Extract time of day
daytime_ts = format(as.POSIXct(TS), "%H:%M:%S"),
daytime_last_ts = format(as.POSIXct(last_TS), "%H:%M:%S")
) %>%
mutate(
hour_of_day = hour(hms(daytime_ts)) +
minute(hms(daytime_ts)) / 60 +
second(hms(daytime_ts)) / 3600,
last_hour_of_day = hour(hms(daytime_last_ts)) +
minute(hms(daytime_last_ts)) / 60 +
second(hms(daytime_last_ts)) / 3600
)  %>%
ungroup() %>%
dplyr::select(username,sources,quartile,message_id, delta_count, posted,TS,time_posted,time_ts, time_last_ts, hour_of_day)
usernames <- reaction_data |>
distinct(username) |>
pull(username)
sources <- reaction_data |>
distinct(sources) |>
pull(sources)
con <- connect_db()
reaction_data <- dbGetQuery(
con,
sprintf(
"SELECT CH.username, RH.message_id, count, RH.TS, posted, IF(RH.reaction_emo = 'VIEWS','VIEWS',HEX(RH.reaction_emo)) AS emo, CH.quartile, CH.sources
FROM reactions_history RH
INNER JOIN messages M
ON RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
INNER JOIN channels_info CH
ON RH.channel_id = CH.channel_id
#     WHERE RH.reaction_emo = 'VIEWS'
AND posted > '%s'
AND posted < '%s'
AND TS <= '%s'
#     AND RH.message_id = 7621
",
anal_start_str, last_message_str, anal_end_str
)
)
# catalogue <- get_catalogue(con)
dbDisconnect(con)
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(delta_count = count - lag(count, default = 0),
last_TS = lag(TS))  %>%
ungroup()
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(
posted_local = with_tz(force_tz(as.POSIXct(posted, tz = "GMT"), "GMT"), "Europe/Prague"),
time_posted = as.numeric(difftime(as.POSIXct(posted_local), anal_start, units = "hours")) ,
time_ts = as.numeric(difftime(as.POSIXct(TS), anal_start, units = "hours")))%>%
mutate(
time_last_ts = ifelse(is.na(last_TS),time_posted, as.numeric(difftime(as.POSIXct(last_TS), anal_start, units = "hours"))),
#    daytime = format(as.POSIXct(posted_local), "%H:%M:%S"),  # Extract time of day
daytime_ts = format(as.POSIXct(TS), "%H:%M:%S"),
daytime_last_ts = format(as.POSIXct(last_TS), "%H:%M:%S")
) %>%
mutate(
hour_of_day = hour(hms(daytime_ts)) +
minute(hms(daytime_ts)) / 60 +
second(hms(daytime_ts)) / 3600,
last_hour_of_day = hour(hms(daytime_last_ts)) +
minute(hms(daytime_last_ts)) / 60 +
second(hms(daytime_last_ts)) / 3600
)  %>%
ungroup() %>%
dplyr::select(username,sources,quartile,message_id, delta_count, posted,TS,time_posted,time_ts, time_last_ts, hour_of_day)
usernames <- reaction_data |>
distinct(username) |>
pull(username)
sources <- reaction_data |>
distinct(sources) |>
pull(sources)
quartiles <- reaction_data |>
distinct(quartile) |>
pull(quartile)
quartiles
sources
library(dplyr)
# Function to get and normalize hourly predictions, with consistent factor levels
get_hourly_levels <- function(model) {
hour_seq <- 0:23 + 0.5
hour_df <- data.frame(
hour_of_day = hour_seq,
hour_bin = cut(hour_seq, breaks = 0:24, right = FALSE, include.lowest = TRUE)
)
# Restrict levels to only those used in training
used_levels <- model$xlevels$hour_bin
hour_df$hour_bin <- factor(hour_df$hour_bin, levels = used_levels)
pred <- predict(model, newdata = hour_df, se.fit = TRUE)
values <- setNames(pred$fit, used_levels)
ses <- setNames(pred$se.fit, used_levels)
list(values = values, se = ses)
}
# Get unique usernames
compute_seasonality <- function(anal_num, views = TRUE) {
if(anal_num == 0) {
names <- c("ALL")
} else if( anal_num == 1) {
names <- usernames
}
# Main loop: create a named list of hour_df-style data frames
hourly_by_channel <- setNames(names, names) |>
lapply(function(ch_id) {
seasonal_data <- reaction_data
if(views) {
seasonal_data <-seasonal_data |>
filter(emo == "VIEWS")
} else {
seasonal_data <-seasonal_data |>
filter(emo != "VIEWS")
}
if(anal_num == 1)
{
seasonal_data <-seasonal_data |>
filter(username == ch_id)
}
seasonal_data  <-seasonal_data |>
mutate(hour_bin = cut(hour_of_day, breaks = 0:24, right = FALSE, include.lowest = TRUE)) |>
filter(time_ts - time_last_ts < 1)
if (nrow(seasonal_data) < 5 || length(unique(seasonal_data$hour_bin)) < 2) {
return(NULL)
}
model <- lm(delta_count ~ hour_bin, data = seasonal_data)
res <- get_hourly_levels(model)
# normalize only on non-NA estimates
norm_c <- 1 / sum(res$values, na.rm = TRUE) * length(res$values)
values <- res$values * norm_c
ses <- res$se * norm_c
data.frame(
hour = 0:23,
hour_bin = cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE),
estimate = as.numeric(values[as.character(cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE))]),
se = as.numeric(ses[as.character(cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE))])
) |>
mutate(
ci_lower = estimate - 1.96 * se,
ci_upper = estimate + 1.96 * se
)
})
hourly_long_df <- hourly_by_channel |>
discard(is.null) |>
imap_dfr(~ mutate(.x, username = .y))
p <- ggplot(hourly_long_df, aes(x = factor(hour), y = estimate)) +
geom_col(fill = "steelblue", width = 0.9) +
geom_errorbar(
aes(ymin = ci_lower, ymax = ci_upper),
width = 0.25,
color = "black"
) +
facet_wrap(~ username, scales = "free_y") +
labs(
title = "Hourly Reaction Rate Estimates by Channel",
x = "Hour of Day",
y = "Normalized Estimate"
) +
theme_minimal() +
theme(
strip.text = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
return(p)
}
p<- compute_seasonality(0)
con <- connect_db()
reaction_data <- dbGetQuery(
con,
sprintf(
"SELECT CH.username, RH.message_id, count, RH.TS, posted, IF(RH.reaction_emo = 'VIEWS','VIEWS',HEX(RH.reaction_emo)) AS emo, CH.quartile, CH.sources
FROM reactions_history RH
INNER JOIN messages M
ON RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
INNER JOIN channels_info CH
ON RH.channel_id = CH.channel_id
#     WHERE RH.reaction_emo = 'VIEWS'
AND posted > '%s'
AND posted < '%s'
AND TS <= '%s'
#     AND RH.message_id = 7621
",
anal_start_str, last_message_str, anal_end_str
)
)
# catalogue <- get_catalogue(con)
dbDisconnect(con)
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(delta_count = count - lag(count, default = 0),
last_TS = lag(TS))  %>%
ungroup()
reaction_data <- reaction_data %>%
arrange(username, message_id, TS) %>%
group_by(username, message_id) %>%
mutate(
posted_local = with_tz(force_tz(as.POSIXct(posted, tz = "GMT"), "GMT"), "Europe/Prague"),
time_posted = as.numeric(difftime(as.POSIXct(posted_local), anal_start, units = "hours")) ,
time_ts = as.numeric(difftime(as.POSIXct(TS), anal_start, units = "hours")))%>%
mutate(
time_last_ts = ifelse(is.na(last_TS),time_posted, as.numeric(difftime(as.POSIXct(last_TS), anal_start, units = "hours"))),
#    daytime = format(as.POSIXct(posted_local), "%H:%M:%S"),  # Extract time of day
daytime_ts = format(as.POSIXct(TS), "%H:%M:%S"),
daytime_last_ts = format(as.POSIXct(last_TS), "%H:%M:%S")
) %>%
mutate(
hour_of_day = hour(hms(daytime_ts)) +
minute(hms(daytime_ts)) / 60 +
second(hms(daytime_ts)) / 3600,
last_hour_of_day = hour(hms(daytime_last_ts)) +
minute(hms(daytime_last_ts)) / 60 +
second(hms(daytime_last_ts)) / 3600
)  %>%
ungroup() %>%
dplyr::select(username,sources,quartile,message_id,emo, delta_count, posted,TS,time_posted,time_ts, time_last_ts, hour_of_day)
usernames <- reaction_data |>
distinct(username) |>
pull(username)
sources <- reaction_data |>
distinct(sources) |>
pull(sources)
quartiles <- reaction_data |>
distinct(quartile) |>
pull(quartile)
library(dplyr)
# Function to get and normalize hourly predictions, with consistent factor levels
get_hourly_levels <- function(model) {
hour_seq <- 0:23 + 0.5
hour_df <- data.frame(
hour_of_day = hour_seq,
hour_bin = cut(hour_seq, breaks = 0:24, right = FALSE, include.lowest = TRUE)
)
# Restrict levels to only those used in training
used_levels <- model$xlevels$hour_bin
hour_df$hour_bin <- factor(hour_df$hour_bin, levels = used_levels)
pred <- predict(model, newdata = hour_df, se.fit = TRUE)
values <- setNames(pred$fit, used_levels)
ses <- setNames(pred$se.fit, used_levels)
list(values = values, se = ses)
}
# Get unique usernames
compute_seasonality <- function(anal_num, views = TRUE) {
if(anal_num == 0) {
names <- c("ALL")
} else if( anal_num == 1) {
names <- usernames
}
# Main loop: create a named list of hour_df-style data frames
hourly_by_channel <- setNames(names, names) |>
lapply(function(ch_id) {
seasonal_data <- reaction_data
if(views) {
seasonal_data <-seasonal_data |>
filter(emo == "VIEWS")
} else {
seasonal_data <-seasonal_data |>
filter(emo != "VIEWS")
}
if(anal_num == 1)
{
seasonal_data <-seasonal_data |>
filter(username == ch_id)
}
seasonal_data  <-seasonal_data |>
mutate(hour_bin = cut(hour_of_day, breaks = 0:24, right = FALSE, include.lowest = TRUE)) |>
filter(time_ts - time_last_ts < 1)
if (nrow(seasonal_data) < 5 || length(unique(seasonal_data$hour_bin)) < 2) {
return(NULL)
}
model <- lm(delta_count ~ hour_bin, data = seasonal_data)
res <- get_hourly_levels(model)
# normalize only on non-NA estimates
norm_c <- 1 / sum(res$values, na.rm = TRUE) * length(res$values)
values <- res$values * norm_c
ses <- res$se * norm_c
data.frame(
hour = 0:23,
hour_bin = cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE),
estimate = as.numeric(values[as.character(cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE))]),
se = as.numeric(ses[as.character(cut(0:23 + 0.5, breaks = 0:24, right = FALSE, include.lowest = TRUE))])
) |>
mutate(
ci_lower = estimate - 1.96 * se,
ci_upper = estimate + 1.96 * se
)
})
hourly_long_df <- hourly_by_channel |>
discard(is.null) |>
imap_dfr(~ mutate(.x, username = .y))
p <- ggplot(hourly_long_df, aes(x = factor(hour), y = estimate)) +
geom_col(fill = "steelblue", width = 0.9) +
geom_errorbar(
aes(ymin = ci_lower, ymax = ci_upper),
width = 0.25,
color = "black"
) +
facet_wrap(~ username, scales = "free_y") +
labs(
title = "Hourly Reaction Rate Estimates by Channel",
x = "Hour of Day",
y = "Normalized Estimate"
) +
theme_minimal() +
theme(
strip.text = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
return(p)
}
p<- compute_seasonality(0)
p
p<- compute_seasonality(0,TRUE)
p
p<- compute_seasonality(0,FALSE)
p
