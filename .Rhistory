interaction_data_rel <- interaction_data_base %>%
mutate(percent_received = n / dest_msg_count) %>%
select(src_username, dest_username, percent_received) %>%
mutate(percent_received = if_else(percent_received < 0.001, 0, percent_received))
# Step 2: Transform interaction_data_rel into a matrix
interaction_matrix_rel <- interaction_data_rel %>%
pivot_wider(names_from = dest_username, values_from = percent_received, values_fill = list(percent_received = 0)) %>%
column_to_rownames(var = "src_username") %>%
as.matrix()
# Check the result
# Ensure row names are set and convert to matrix
output_file_matrix_rel_csv <- file.path(output_folder, paste0( islandid,"_interaction_matrix_rel", ".csv"))
write.csv(interaction_matrix_rel, output_file_matrix_rel_csv, row.names = TRUE)
# Step 3: Convert the matrix into an igraph object
interaction_graph_rel <- graph_from_adjacency_matrix(interaction_matrix_rel, mode = "directed", weighted = TRUE)
interaction_graph_rel <- delete_vertices(interaction_graph_rel, which(degree(interaction_graph_rel) == 0))
# Print debugging info
cat("Number of vertices:", vcount(interaction_graph_rel), "\n")
cat("Number of edges:", ecount(interaction_graph_rel), "\n")
# Check if there are any edges with weights greater than 0
print(E(interaction_graph_rel)$weight)
# Step 4: Simplify the plot and adjust width scaling
plot_rel <- ggraph(interaction_graph_rel, layout = "fr") +
geom_edge_link(aes(edge_alpha = weight, edge_width = weight),
arrow = arrow(length = unit(3, 'mm')), # Define arrow size for visibility
end_cap = circle(3, 'mm')) + # Adjust end cap for clarity
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 6) + # Increase label size to 6
theme_void()  # Step 5: Save the plot as PNG
output_path_rel <- file.path(output_folder, paste0(islandid,"_interaction_graph_rel.png"))
ggsave(output_path_rel, plot = plot_rel, width = 10, height = 8, dpi = 300)
top_channels <- top_channels %>%
mutate(
accepted = forwarded_to / msg_count,
sent = forwarded_from / msg_count,
reacted = reaction_count / msg_count
)
# Save to CSV file
output_file_csv <- file.path(output_folder, paste0(islandid, "_top_20_channels_by_reach.csv"))
write.csv(top_channels, output_file_csv, row.names = FALSE)
# Convert data into long format for ggplot2 compatibility
plot_data <- top_channels %>%
mutate(
username = str_trunc(username, 25),    # Trim usernames to 25 characters
accepted = forwarded_to / msg_count,
sent = forwarded_from / msg_count
) %>%
select(username, accepted, sent) %>%
pivot_longer(cols = c("accepted", "sent"), names_to = "type", values_to = "value") %>%
mutate(value = if_else(type == "accepted", -value, value)) # accepted as negative
# Create the plot with larger labels
plot_dist <- ggplot(plot_data, aes(x = username, y = value, fill = type)) +
geom_bar(stat = "identity") +
scale_y_continuous(labels = abs, expand = c(0, 0)) +
labs(
x = "Username",
y = "Ratio",
fill = "Type",
title = "Sent (above) and Accepted (below) Ratios by Username"
) +
theme_minimal() +
theme(
axis.text.x = element_text(size = 14, angle = 45, hjust = 1, vjust = 1), # Larger, skewed x-axis labels
axis.text.y = element_text(size = 12),                                  # Larger y-axis labels
legend.position = "top"
)
# Create the plot
output_file_png <- file.path(output_folder, paste0(islandid,"_reach_dist.png"))
ggsave(output_file_png, plot = plot_dist, width = 10, height = 6)
top_10_channels <- catalogue_data %>% top_n(num_channels, reach_own)%>%
arrange(desc(reach_own))
others_channels <- catalogue_data %>%
filter(!username %in% top_10_channels$username) %>%
summarise(username = "Other", reach_own = sum(reach_own))
reach_summary <- bind_rows(top_10_channels, others_channels)
output_file_csv <- file.path(output_folder, paste0(islandid,"_reach_summary.csv"))
write.csv(reach_summary, output_file_csv, row.names = FALSE)
# Vykreslení koláčového grafu pro 'sum_members'
plot_sum_members <- ggplot(reach_summary, aes(x = "", y = reach_own, fill = username)) +
geom_bar(stat = "identity", width = 1) +
coord_polar(theta = "y") +
labs(
title = "Channels by Reach",
x = NULL,
y = NULL
) +
theme_minimal() +
theme(axis.text.x = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) +
geom_text(aes(label = paste0(username, ": ", scales::number(reach_own / 1000000, accuracy = 1),"M")),
position = position_stack(vjust = 0.5), check_overlap = TRUE)
# Uložení grafu 'sum_members'
output_file_png <- file.path(output_folder, paste0(islandid,"_reach_summary.png"))
ggsave(output_file_png, plot = plot_sum_members, width = 10, height = 6)
# Close the database connectionint
dbDisconnect(con)
chnumber <- 15
inisig_hist <- 20
test_treshold <- 2.5
orange_ratio <- 3
list_threshold <- 5
min_src_size <- 50
source("defs.R")
con <- connect_db()
sources <-  dbGetQuery(con,paste0("SELECT  src_channel_id,  COUNT(*) AS row_count   FROM
messages
WHERE
channel_id = ", this_channel_id, " AND src_channel_id IS NOT NULL
GROUP BY
src_channel_id
HAVING
row_count > ", min_src_size," ORDER BY  row_count DESC"))
sources <-  dbGetQuery(con,paste0("SELECT  src_channel_id,  COUNT(*) AS row_count   FROM
messages
WHERE
channel_id = ", this_channel_id, " AND src_channel_id IS NOT NULL
GROUP BY
src_channel_id
HAVING
row_count > ", min_src_size," ORDER BY  row_count DESC"))
chnumber <- 15
inisig_hist <- 20
test_treshold <- 2.5
orange_ratio <- 3
list_threshold <- 5
min_src_size <- 50
source("defs.R")
con <- connect_db()
# sources <-  dbGetQuery(con,paste0("SELECT  src_channel_id,  COUNT(*) AS row_count   FROM
#  messages
#   WHERE
#   channel_id = ", this_channel_id, " AND src_channel_id IS NOT NULL
#   GROUP BY
#  src_channel_id
#  HAVING
#  row_count > ", min_src_size," ORDER BY  row_count DESC"))
compute_mode <- function(x) {
uniq_x <- unique(x)            # Get unique values
uniq_x[which.max(tabulate(match(x, uniq_x)))]  # Find the most frequent value
}
# Connect to MySQL database
# Query the database for relevant data
# catalogue_data <- dbGetQuery(con, "SELECT channel_id, username, CONVERT(name USING ASCII) as name, subscribers, catalogue_count,
#             msg_count, reaction_count, forwarded_from, forwarded_to, lang FROM channels_info WHERE (source = 'slerka')")
catalogue_data <- dbGetQuery(con, "SELECT channel_id, username, CONVERT(name USING ASCII) as name, subscribers, catalogue_count,
msg_count, reaction_count, forwarded_from, forwarded_to, lang FROM channels_info WHERE (lang = 'CZECH' or lang = 'SLOVAK')")
catalogue_data <- catalogue_data %>%
mutate(subscribers = if_else(
str_detect(subscribers, "K"),
as.numeric(str_replace(subscribers, "K", "")) * 1000,
if_else(
str_detect(subscribers, "M"),as.numeric(str_replace(subscribers, "M", "")) * 1000000, as.numeric(subscribers))
))
catalogue_data <- catalogue_data %>%
mutate(reach_own = subscribers * msg_count)
top_channels_reach <- catalogue_data %>%
arrange(desc(reach_own)) %>%
head(chnumber)
top_channels_to <- catalogue_data %>%
arrange(desc(forwarded_to)) %>%
head(chnumber)
top_channels_from <- catalogue_data %>%
arrange(desc(forwarded_from)) %>%
head(chnumber)
top_channels <- bind_rows(top_channels_reach, top_channels_to, top_channels_from) %>%
distinct()
global_catalogue_table <- data.frame()
# Loop through each row in top_channels
forwards_data <- dbGetQuery(con, "SELECT * from forwards")
forwards_data <- forwards_data %>%
inner_join(top_channels, by = c("channel_id" = "channel_id")) %>%
rename(dst_username = username )  %>%
inner_join(top_channels, by = c("src_channel_id" = "channel_id")) %>%
rename(src_username = username )
forwards_data <- forwards_data %>%
mutate(log_forward_time = log10(as.numeric(difftime(posted, src_posted, units = "secs"))),
cat_forward_time = ceiling(log_forward_time*2)/2) %>%
select(src_username, dst_username, log_forward_time,cat_forward_time)
output_csv <- paste0(output_folder, islandid, "_forward_data.csv")
write.csv(forwards_data,output_csv)
summary_table <- forwards_data %>%
group_by(src_username, dst_username) %>%            # Group by src_channel_id and channel_id
filter(n() >= list_threshold) %>%                               # Keep groups with at least 10 records
summarise(
mean_forward_time = mean(log_forward_time, na.rm = TRUE), # Mean forward time
mode_forward_time = compute_mode(cat_forward_time), # Mean forward time
sd_forward_time = sd(log_forward_time, na.rm = TRUE),     # Standard deviation of forward time
n_records = n(),
# Number of records
ratio_below_t = sum(log_forward_time < test_treshold, na.rm = TRUE) / n(),
# Number of records
.groups = "drop"                                      # Ungroup after summarising
)  %>%
mutate( left_ratio = ratio_below_t / pnorm(test_treshold, mean = mean_forward_time, sd = sd_forward_time, lower.tail = TRUE, log.p = FALSE),
bar_color = if_else(n_records < inisig_hist,"grey",
if_else(mode_forward_time <= 2, "black",
if_else(left_ratio > orange_ratio,"red", "darkgreen")))
)
output_csv <- paste0(output_folder, islandid, "_forward_time_summary.csv")
write.csv(summary_table,output_csv)
# Filter data for combinations with at least 50 rows
filtered_data <- forwards_data %>%
group_by(src_username, dst_username) %>%
filter(n() >= list_threshold) %>%
ungroup()
output_pdf <- paste0(output_folder, islandid, "_forward_time_histograms_grid.pdf")
# Add a flag for small datasets if needed
merged_data <- filtered_data %>%
left_join(summary_table %>%
select(src_username, dst_username, mode_forward_time, n_records,left_ratio, bar_color),
by = c("src_username", "dst_username"))
# Add a flag for small datasets if needed
if (nrow(merged_data) == 0) {
stop("No combinations with at least 50 rows found.")
}
# Create a faceted plot with color based on mode_forward_time
# Load necessary libraries
# Define output folder for PDF
# Add missing '1' for cat_forward_time for each combination
write.csv(merged_data,paste0(output_folder,"data_for_grid.csv"))
output_pdf <- paste0(output_folder, islandid, "_forward_time_histograms_colored_start1.pdf")
# Create a faceted plot with forced x-axis starting from 1 and colored bars
p <- ggplot(merged_data, aes(x = cat_forward_time, fill = bar_color)) +
geom_histogram(stat = "count", binwidth = 1, color = "white", alpha = 0.7) +
scale_fill_manual(values = c("red" = "red", "darkgreen" = "darkgreen", "grey" = "grey", "black" = "black")) +
scale_x_continuous(breaks = 0:8, limits = c(0, 8)) +
labs(
x = "t",
y = "freq"
) +
theme_minimal() +
facet_grid(rows = vars(src_username), cols = vars(dst_username), scales = "free") +
theme(
strip.text.x = element_text(angle = 45, hjust = 1),
strip.text.y = element_text(angle = 0)
)
# Save the plot to a PDF
ggsave(output_pdf, plot = p, width = 16, height = 12)
dbDisconnect(con)
source("defs.R")
library(lubridate)
library(purrr)
anal_start <- as.POSIXct("2024-12-09 20:00:00")
last_messages<- as.POSIXct("2024-12-28 20:00:00")
anal_end <- as.POSIXct("2025-01-01 20:00:00")
con <- connect_db()
top_emoticons <- init_emos(con)
library(RMySQL)
library(dplyr)
# Define output folder for CSV and PDF
output_folder <- "out/"
# Connect to MySQL database
con <- dbConnect(
MySQL(),
host = "bethel.utia.cas.cz",
user = "jak",
password = "jaknajaka",
dbname = "telegram_full"
)
# Query the database for relevant data
emoticons <- dbGetQuery(
con,
"SELECT unified_emo, emo_name, HEX(unified_emo) AS emo, SUM(count) AS count
FROM message_reactions_info
WHERE lang='CZECH' OR lang='SLOVAK'
GROUP BY unified_emo
ORDER BY count DESC"
)
source("defs.R")
con <- connect_db()
# Query the database for relevant data
emoticons <- dbGetQuery(
con,
"SELECT unified_emo, emo_name, HEX(unified_emo) AS emo, SUM(count) AS count
FROM message_reactions_info
WHERE lang='CZECH' OR lang='SLOVAK'
GROUP BY unified_emo
ORDER BY count DESC"
)
source("defs.R")
con <- connect_db()
# Query the database for relevant data
emoticons <- dbGetQuery(
con,
"SELECT unified_emo, emo_name, HEX(unified_emo) AS emo, SUM(count) AS count
FROM message_reactions_info
WHERE lang='CZECH' OR lang='SLOVAK'
GROUP BY unified_emo
ORDER BY count DESC"
)
# Calculate cumulative percentage and filter for top 99%
emoticons_top <- emoticons %>%
mutate(cumulative_sum = cumsum(count), # Cumulative sum
total_sum = sum(count),         # Total sum
cumulative_percent = cumulative_sum / total_sum * 100) %>%
filter(cumulative_percent <= 99)      # Select rows within 99% cumulative total
# If you want to save the output as a CSV
write.csv(emoticons_top, file.path(output_folder, "top_emoticons_99.csv"), row.names = FALSE)
# Disconnect from the database
dbDisconnect(con)
source("defs.R")
library(lubridate)
library(purrr)
anal_start <- as.POSIXct("2024-12-09 20:00:00")
last_messages<- as.POSIXct("2024-12-28 20:00:00")
anal_end <- as.POSIXct("2025-01-01 20:00:00")
con <- connect_db()
top_emoticons <- init_emos(con)
reaction_data <- dbGetQuery(con, "SELECT RH.channel_id,  RH.message_id, HEX(RH.reaction_emo) as emo, count, RH.TS,
posted from reactions_history RH INNER JOIN messages M
ON(RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
)")
#AND RH.channel_id = 1833277468  and (msg_id = 39335 or msg_id = 39336)
reaction_data <- reaction_data %>%
filter(posted > anal_start & posted < last_messages)
reaction_data <- reaction_data %>%
arrange(channel_id, message_id, emo, TS) %>%
group_by(channel_id, message_id) %>%
mutate(
time_end = difftime(TS, posted, units = "hours")
#,
#    time_diff = ifelse(is.na(lag(TS)) ,
#                       difftime(TS, posted, units = "days"),
#                     difftime(TS, lag(TS), units = "days")),
#    count_diff = count - lag(count, default = 0)
) %>%
ungroup()
reaction_data <- reaction_data %>%
group_by(channel_id, message_id) %>%
mutate(censor_point = difftime(anal_end,posted, units = "hours")) %>%
ungroup()
time_grid <- reaction_data %>%
distinct(channel_id, message_id, emo,censor_point) %>%
mutate(hourly_times = map(censor_point, ~ seq(1, floor(.x), by = 1))) %>%
unnest(hourly_times) %>%
rename(hourly_end = hourly_times)
t
# Calculate time differences and count increments
# Calculate time differences and count increments, including hourly increases
result <- reaction_data %>%
mutate(hourly_end = floor(as.numeric(time_end))) %>%
group_by(channel_id, message_id, emo, hourly_end) %>%
slice_max(order_by = TS, n = 1, with_ties = FALSE) %>%  # Keep the last row based on TS
ungroup() %>%
select(channel_id,message_id, emo, hourly_end,count)%>%
right_join(time_grid, by = c("channel_id", "message_id", "emo", "hourly_end")) %>%
arrange(channel_id, message_id, emo, hourly_end) %>%
group_by(channel_id, message_id, emo) %>%
fill(count, .direction = "down") %>% # Fill missing values in counts
mutate(hourly_increase = count - lag(count, default = 0)) %>% # Calculate hourly increase
ungroup()
# Write the result to a CSV
write.csv(result, paste0(output_folder,islandid, "_times.csv"))
result$emo_UTF8 <- hex_to_text(result$emo)
# Iterate through each channel_id and message_id combination
result %>%
group_by(channel_id, message_id) %>%
group_walk(~ {
data <- .x
channel_id <- .y$channel_id
message_id <- .y$message_id
print(paste("Processing Channel:", channel_id, "Message:", message_id)) # Debug info
# Create the bar graph
p <- ggplot(data, aes(x = hourly_end, y = hourly_increase, fill = emo_UTF8)) +
geom_bar(stat = "identity", position = "stack") +
labs(
title = paste0("Channel: ", channel_id,
" | Message: ", message_id),
x = "Hour",
y = "Hourly Increase",
fill = "Emoticon"
) +
theme_minimal()
# Save the plot to a PNG file
output_file <- paste0(
graph_output_folder, islandid,
"_channel_", channel_id,
"_message_", message_id, ".png"
)
ggsave(output_file, plot = p, width = 10, height = 6)
})
source("defs.R")
library(lubridate)
library(purrr)
anal_start <- as.POSIXct("2024-12-09 20:00:00")
last_messages<- as.POSIXct("2024-12-28 20:00:00")
anal_end <- as.POSIXct("2025-01-01 20:00:00")
con <- connect_db()
top_emoticons <- init_emos(con)
reaction_data <- dbGetQuery(con, "SELECT RH.channel_id,  RH.message_id, HEX(RH.reaction_emo) as emo, count, RH.TS,
posted from reactions_history RH INNER JOIN messages M
ON(RH.channel_id = M.channel_id AND RH.message_id = M.msg_id
)")
#AND RH.channel_id = 1833277468  and (msg_id = 39335 or msg_id = 39336)
reaction_data <- reaction_data %>%
filter(posted > anal_start & posted < last_messages)
reaction_data <- reaction_data %>%
arrange(channel_id, message_id, emo, TS) %>%
group_by(channel_id, message_id) %>%
mutate(
time_end = difftime(TS, posted, units = "hours")
#,
#    time_diff = ifelse(is.na(lag(TS)) ,
#                       difftime(TS, posted, units = "days"),
#                     difftime(TS, lag(TS), units = "days")),
#    count_diff = count - lag(count, default = 0)
) %>%
ungroup()
reaction_data <- reaction_data %>%
group_by(channel_id, message_id) %>%
mutate(censor_point = difftime(anal_end,posted, units = "hours")) %>%
ungroup()
time_grid <- reaction_data %>%
distinct(channel_id, message_id, emo,censor_point) %>%
mutate(hourly_times = map(censor_point, ~ seq(1, floor(.x), by = 1))) %>%
unnest(hourly_times) %>%
rename(hourly_end = hourly_times)
t
# Calculate time differences and count increments
# Calculate time differences and count increments, including hourly increases
result <- reaction_data %>%
mutate(hourly_end = floor(as.numeric(time_end))) %>%
group_by(channel_id, message_id, emo, hourly_end) %>%
slice_max(order_by = TS, n = 1, with_ties = FALSE) %>%  # Keep the last row based on TS
ungroup() %>%
select(channel_id,message_id, emo, hourly_end,count)%>%
right_join(time_grid, by = c("channel_id", "message_id", "emo", "hourly_end")) %>%
arrange(channel_id, message_id, emo, hourly_end) %>%
group_by(channel_id, message_id, emo) %>%
fill(count, .direction = "down") %>% # Fill missing values in counts
mutate(hourly_increase = count - lag(count, default = 0)) %>% # Calculate hourly increase
ungroup()
# Write the result to a CSV
write.csv(result, paste0(output_folder,islandid, "_times.csv"))
result$emo_UTF8 <- hex_to_text(result$emo)
# Iterate through each channel_id and message_id combination
result %>%
group_by(channel_id, message_id) %>%
group_walk(~ {
data <- .x
channel_id <- .y$channel_id
message_id <- .y$message_id
print(paste("Processing Channel:", channel_id, "Message:", message_id)) # Debug info
# Create the bar graph
p <- ggplot(data, aes(x = hourly_end, y = hourly_increase, fill = emo_UTF8)) +
geom_bar(stat = "identity", position = "stack") +
labs(
title = paste0("Channel: ", channel_id,
" | Message: ", message_id),
x = "Hour",
y = "Hourly Increase",
fill = "Emoticon"
) +
theme_minimal()
# Save the plot to a PNG file
output_file <- paste0(
graph_output_folder, islandid,
"_channel_", channel_id,
"_message_", message_id, ".png"
)
ggsave(output_file, plot = p, width = 10, height = 6)
})
channels <- dbGetQuery(con, "SELECT * FROM channels")
mychannels <- result  %>% distinct(channel_id) %>%
left_join(channels,by = c("channel_id" = "id" ))
for (i in 1:nrow(mychannels)) {
username <- mychannels$username[i]
html_text <-result %>%
filter(channel_id == mychannels$channel_id[i]) %>%
group_by(channel_id, message_id) %>%
mutate(
message_url = paste0("https://t.me/", username, "/", message_id),
embed_code = paste0(
"<script async src=\"https://telegram.org/js/telegram-widget.js?22\" data-telegram-post=\"",
username, "/", message_id, "\" data-width=\"100%\"></script> "
),
message_link = sprintf("<a href='%s' target='_blank'>link</a>", message_url),
censor_point = round(censor_point),
info_code = paste0(
"URL ", message_link,
"<br>Len ", censor_point
),
image_cell = paste0("<img src=\"",graph_subfolder,"channel_", unique(channel_id),
"_message_", unique(message_id), ".png\" alt=\"histogram\" width=\"600\">")
) %>%
ungroup()  %>%
distinct(info_code, embed_code,image_cell)
library(kableExtra)
html_table <- html_text %>%
kable(
format = "html",
escape = FALSE,
col.names = c("Info", "Message", "Emos")
) %>%
kable_styling(
full_width = FALSE,
bootstrap_options = c("striped", "hover", "condensed")
) %>%
column_spec(1, extra_css = "vertical-align: top;") %>%
column_spec(2, extra_css = "vertical-align: top; width: 40%;") %>% # Set consistent width
column_spec(3, extra_css = "vertical-align: top; width: 40%;")     # Set consistent width
# Step 2: Add the concatenated URL column
# Assuming 'username' and 'message_id' are columns in the original dataset or grouped_data
html_content <- paste0(
"<!DOCTYPE html><html><head><title>", username, "</title></head><body>",
"<h1>", username, "</h1>",
as.character(html_table),
"</body></html>"
)
output_file <- file.path(output_folder, paste0(islandid,"_",username, "_emo_dists.html"))
writeLines(html_content, output_file)
}
dbDisconnect(con)
